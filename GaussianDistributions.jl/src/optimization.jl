using OptimizationAlgorithms
const Optimization = OptimizationAlgorithms

# optimization of kernel hyperparameters of Gaussian Process
# could add optimization w.r.t. leave-one-out loss
# TODO: optimization of noise variance
# IDEA: pass inference method (Cholesky, Iterative, Stochastic, ...)
optimize(k, Î¸, x, y, ÏƒÂ²::Real) = optimize(k, Î¸, x, ð’©(y, ÏƒÂ²))
function optimize(k, Î¸::AbstractVector, x::AbstractVector,
                  y::Union{AbstractVector, Multivariate})
    optimize!(k, copy(Î¸), x, y)
end
optimize!(k, Î¸, x, y, ÏƒÂ²::Real) = optimize!(k, Î¸, x, ð’©(y, ÏƒÂ²))
function optimize!(k, Î¸::AbstractVector, x::AbstractVector,
                   y::Union{AbstractVector, Multivariate})
    val(Î¸) = nlml(k(Î¸), x, y) # make this "safe"?
    function valdir(Î¸)
        v, g = value_gradient(nlml, k, Î¸, x, y) # need to add tol if y is Multivariate
        g .*= -1
        v, g
    end
    # optimization based on L-BFGS with m stages
    m = 5
    D = Optimization.CustomDirection(val, valdir, Î¸)
    D = Optimization.LBFGS(D, Î¸, m, check = false)
    # scaling proposed Dection to unit length before doing line search,
    # avoids jumping to sub-optimal and very large parameter vectors
    D = Optimization.UnitDirection(D)
    # to accept a set, do line search for step size to ensure decrease
    D = Optimization.DecreasingStep(D, Î¸)
    # finally, find fixed point of the iterations
    _, t = Optimization.fixedpoint!(D, Î¸)
    return Î¸
end

# here, k(Î¸) is assumed to be a kernel function
# computes both value and gradient
function value_gradient(::typeof(nlml), k, Î¸::AbstractVector, x::AbstractVector,
                        y::Union{AbstractVector, Multivariate})
    val, push = pushforward(nlml, k(Î¸), x, y)

    âˆ‡ = (x, y) -> reshape(ForwardDiff.gradient(z->k(z)(x, y), Î¸), :, 1)
    G = gramian(âˆ‡, x) # lazily evaluated matrix of kernel gradients
    G_matofvec = G.A
    G = matvec2vecmat(G_matofvec) # casts G to a vector of dense matrices. IDEA: make lazy!

    grad = similar(Î¸)
    @threads for i in eachindex(grad) # parallelizing here is efficient
        grad[i] = push(G[i])
    end
    return val, grad
end

function gradient(::typeof(nlml), k, Î¸::AbstractVector,
                    x::AbstractVector, y::Union{AbstractVector, Multivariate})
    value_gradient(nlml, k, Î¸, x, y)[2]
end
using CovarianceFunctions: Gramian
matvec2vecmat(A::Gramian{<:AbstractVecOrMat}) = matvec2vecmat(Matrix(A))
# convert matrix of vectors to vector of matrices
function matvec2vecmat(A::AbstractMatrix{<:AbstractVecOrMat})
    k = length(A[1])
    all(==(k), (length(Ai) for Ai in A)) || throw(DimensionMismatch("component vectors do not have the same size"))
    n, m = size(A)
    B = Vector{Matrix{eltype(A[1])}}(undef, k)
    for i in eachindex(B) # this is still inefficient, because it calculates the gradient multiple times
        B[i] = [a[i] for a in A]
    end
    return B
end

########################### optimizing noise variance ##########################
function value_gradient(::typeof(nlml), log_ÏƒÂ²::Real, K::Factorization, y::AbstractVector)
    n = length(y)
    Î£ = exp(log_ÏƒÂ²)*I(n)
    W = Woodbury(Î£, K)
    val, push = pushforward(nlml, W, y)
    grad = push(Î£)
    return val, grad
end

function optimize(ÏƒÂ²::Real, k, x::AbstractVector, y::AbstractVector; tol::Real = tol)
    # pre-factorize kernel matrix
    n = length(x)
    K = gramian(k, x)
    K = factorize(K, tol = tol)
    function val(log_ÏƒÂ²::Real)
        Î£ = exp(log_ÏƒÂ²)*I(n)
        nlml(Woodbury(Î£, K), y)
    end
    val(Î¸::AbstractVector) = val(Î¸[1])
    function valdir(log_ÏƒÂ²::Real)
        v, g = value_gradient(nlml, log_ÏƒÂ², K, y)
        g = -g
        v, g
    end
    function valdir(Î¸::AbstractVector)
        v, g = valdir(Î¸[1])
        v, [g]
    end
    Î¸ = [log(ÏƒÂ²)]
    D = Optimization.CustomDirection(val, valdir, Î¸)
    D = Optimization.LBFGS(D, Î¸, 2, check = false) # does this reduce to secant?
    D = Optimization.UnitDirection(D)
    D = Optimization.DecreasingStep(D, Î¸)
    Optimization.fixedpoint!(D, Î¸)
    return ÏƒÂ² = exp(Î¸[1])
end
