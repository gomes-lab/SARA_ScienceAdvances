module GaussianDistributions
# there are fundamental operations with Gaussian distributions:
# 1. marginals
# 2. conditionals
# 3. linear functionals
# 4. sampling
# 5. optimizing hyper-parameters of Gaussian Processes
# this package provides convenient and efficient implementations for these operations
function marginal end
function conditional end

using LinearAlgebra
using LinearAlgebra: checksquare

using Base.Threads
using StaticArrays
using FillArrays
using Statistics
import StatsBase: sample

using CovarianceFunctions
using CovarianceFunctions: gramian, MercerKernel, MultiKernel, AbstractKernel
Statistics.cov(k::MercerKernel, x::AbstractVector) = gramian(k, x)

using ForwardDiff # necessary for value_gradient in util.jl
using DiffResults

export Gaussian, Univariate, Multivariate, VectorGaussian, ùí©
export mean, cov, var, std, pdf, cdf, nld, nlml
export marginal, conditional, conditional!, sample

using LazyInverse: inverse, Inverse, pseudoinverse
using LazyLinearAlgebra
using LinearAlgebraExtensions: difference, AbstractMatOrFacOrUni, AbstractMatOrFac
using WoodburyIdentity

abstract type AbstractGaussian{M, S} end
Statistics.mean(G::AbstractGaussian) = G.Œº
Statistics.cov(G::AbstractGaussian) = G.Œ£ # ensure Hermitian-ness if Œ£ is a matrix?
const tol = 1e-6 # default error tolerance for matrix factorizations and samples
const VecOfVec{T} = AbstractVector{<:AbstractVector{T}}

############################# Gaussian Distribution ############################
struct Gaussian{M, S} <: AbstractGaussian{M, S}
    Œº::M
    Œ£::S
	function Gaussian(Œº::Real, œÉ¬≤::Real)
		œÉ¬≤ ‚â• 0 ? new{typeof(Œº), typeof(œÉ¬≤)}(float(Œº), œÉ¬≤) : error("œÉ¬≤ < 0")
	end
	function Gaussian(Œº::AbstractVector, Œ£::AbstractMatOrFacOrUni)
		length(Œº) == checksquare(Œ£) || error("dimensions of Œº, Œ£ not compatible")
		new{typeof(Œº), typeof(Œ£)}(Œº, Œ£)
	end
	Gaussian(Œº, Œ£) = new{typeof(Œº), typeof(Œ£)}(Œº, Œ£)
end
const ùí© = Gaussian
# 1-argument input constructors, assuming mean zero
Gaussian() = Gaussian(0., 1.)
# passing variances only
Gaussian(œÉ¬≤::Real) = Gaussian(zero(œÉ¬≤), œÉ¬≤)
Gaussian(œÉ¬≤::AbstractVector) = Gaussian(Diagonal(œÉ¬≤))
Gaussian(Œ£::AbstractMatOrFacOrUni) = Gaussian(Zeros(size(Œ£, 1)), Œ£)
# passing mean and variances
Gaussian(Œº::AbstractVector, œÉ¬≤::Real) = Gaussian(Œº, œÉ¬≤*I(length(Œº)))
Gaussian(Œº::AbstractVector, œÉ¬≤::AbstractVector) = Gaussian(Œº, Diagonal(œÉ¬≤))

zero_mean(x) = zero(eltype(x))
# TODO: make this a mutable static vector
zero_mean(x, d::Int) = zeros(eltype(x), d)
zero_mean(d::Int) = x->zero_mean(x, d)

Gaussian(k) = Gaussian(zero_mean, k)

function Base.length(G::Gaussian)
	if G isa Univariate || G isa Multivariate
		return length(G.Œº)
	else
		return Inf # assumed to be process
	end
end

const UnivariateGaussian{M, S} = Gaussian{<:Real, <:Real}
const Univariate = UnivariateGaussian
const Uni = Univariate

# (G::Univariate)(i::Int) = marginal(G, i)
Base.getindex(G::Univariate, i) = marginal(G, i) # bracket-indexing

const MultivariateGaussian{M, S} = Gaussian{<:AbstractVector, <:AbstractMatOrFacOrUni}
const Multivariate = MultivariateGaussian
const Multi = Multivariate

# (G::Multivariate)(i::Int) = marginal(G, i)
Base.getindex(G::Multivariate, i) = marginal(G, i) # bracket-indexing

# Gaussian is assumed to be process-valued, if neither Uni, nor Multi
(G::Gaussian)(x) = Gaussian(mean(G)(x), cov(G)(x, x))

############################# convenience ######################################
Statistics.cov(G::Multivariate, i, j) = cov(G)[i, j] # view?
Statistics.var(G::Union{Uni, Multi}) = diag(cov(G))
Statistics.std(G::Union{Uni, Multi}) = sqrt.(max.(var(G), 0))

# variance (computes diagonal of covariance), should be specialized where
# more efficient computation is possible
Statistics.var(G::Gaussian) = x->max(G.Œ£(x, x), 0)
Statistics.std(G::Gaussian) = x->sqrt(var(G)(x))
Statistics.mean(G::Gaussian, x::AbstractVector) = marginal(mean(G), x)
# IDEA: allow for randomized approximation of this function akin to "Scaling Gaussian Process Regression with Derivatives"
# Statistics.var(G::Gaussian, x::AbstractVector) = var(G).(x)
function Statistics.var(G::Gaussian, x::AbstractVector)
	œÉ¬≤ = fill(var(G)(x[1]), length(x))
	@threads for i in 2:length(x) # virtually perfect parallel scaling
		œÉ¬≤[i] = var(G)(x[i])
	end
	return œÉ¬≤
end
function Statistics.std(G::Gaussian, x::AbstractVector)
	œÉ = var(G, x)
	@. œÉ = sqrt(max(œÉ, 0))
end



Base.copy(G::Gaussian) = Gaussian(copy(G.Œº), copy(G.Œ£))
function Base.:(==)(G::Gaussian, H::Gaussian)
	return mean(G) == mean(H) && cov(G) == cov(H)
end
function Base.isapprox(G::Gaussian, H::Gaussian; atol = eps(), rtol = eps())
    isapprox(G.Œº, H.Œº, atol = atol, rtol = rtol) && isapprox(G.Œ£, H.Œ£, atol = atol, rtol = rtol)
end

function LinearAlgebra.cholesky(G::Multivariate; tol::Real = tol, check::Bool = false) # low rank via cholesky
	cov(G) isa AbstractMatrix || return G # to prevent calling cholesky on a factorization
	S = Symmetric(cov(G))
    C = cholesky(S, Val(true), tol = tol, check = check)
    if check
		issuccess(C) || throw("pivoted cholesky not successful")
	end
    return Gaussian(mean(G), C)
end
function LinearAlgebra.factorize(G::Multivariate; tol::Real = tol)
	cov(G) isa AbstractMatrix || return G # to prevent calling cholesky on a non-factorizable object
	return cholesky(G, tol = tol)
end

include("util.jl")
include("sample.jl")
include("marginal.jl")
include("conditional.jl")
include("multi.jl")
include("linear.jl")
include("density.jl")
include("optimization.jl")
include("plot.jl")

# non-essential extensions
include("inputnoise.jl")

end # GaussianDistributions
